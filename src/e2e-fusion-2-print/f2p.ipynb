{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c286cbb9",
   "metadata": {},
   "source": [
    "# Fusion2Print End-2-End Enhancement-Embedding Pipeline\n",
    "\n",
    "**Note:** In this notebook we can experiment with \n",
    "\n",
    "- Finetuning hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed64a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pytorch_msssim import ssim\n",
    "from skimage.morphology import skeletonize\n",
    "import random\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from pytorch_msssim import ssim \n",
    "import re\n",
    "import torch.fft as fft\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn.metrics import pairwise_distances, euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f038e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" # change to your gpu or cpu\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_resize(img, size=512):\n",
    "    \"\"\"\n",
    "    Pad to square with black padding, then resize to (size,size).\n",
    "    Works for both PIL.Image and torch.Tensor (C,H,W).\n",
    "    \"\"\"\n",
    "    #  Case 1: PIL Image \n",
    "    if isinstance(img, Image.Image):\n",
    "\n",
    "        w, h = img.size\n",
    "        pad_left = pad_right = pad_top = pad_bottom = 0\n",
    "\n",
    "        if w > h:\n",
    "            diff = w - h\n",
    "            pad_top = diff // 2\n",
    "            pad_bottom = diff - pad_top\n",
    "        else:\n",
    "            diff = h - w\n",
    "            pad_left = diff // 2\n",
    "            pad_right = diff - pad_left\n",
    "\n",
    "        img_padded = TF.pad(img, padding=(pad_left, pad_top, pad_right, pad_bottom), padding_mode=\"constant\", fill=0)\n",
    "        img_resized = TF.resize(img_padded, (size, size))\n",
    "\n",
    "        pad_info = {\n",
    "            \"orig_size\": (h, w),\n",
    "            \"pad\": (pad_top, pad_bottom, pad_left, pad_right),\n",
    "            \"resized_size\": (size, size),\n",
    "        }\n",
    "\n",
    "        return img_resized, pad_info\n",
    "\n",
    "    #  Case 2: torch.Tensor (C,H,W) \n",
    "    elif isinstance(img, torch.Tensor):\n",
    "\n",
    "        if img.ndim != 3:\n",
    "            raise ValueError(f\"Expected tensor of shape (C,H,W), got {img.shape}\")\n",
    "\n",
    "        _, h, w = img.shape\n",
    "        pad_left = pad_right = pad_top = pad_bottom = 0\n",
    "\n",
    "        if w > h:\n",
    "            diff = w - h\n",
    "            pad_top = diff // 2\n",
    "            pad_bottom = diff - pad_top\n",
    "        else:\n",
    "            diff = h - w\n",
    "            pad_left = diff // 2\n",
    "            pad_right = diff - pad_left\n",
    "\n",
    "        img_padded = TF.pad(img, padding=(pad_left, pad_top, pad_right, pad_bottom), value=0)\n",
    "        img_resized = TF.resize(img_padded, (size, size))\n",
    "\n",
    "        pad_info = {\n",
    "            \"orig_size\": (h, w),\n",
    "            \"pad\": (pad_top, pad_bottom, pad_left, pad_right),\n",
    "            \"resized_size\": (size, size),\n",
    "        }\n",
    "\n",
    "        return img_resized, pad_info\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported input type: {type(img)}. Must be PIL.Image or torch.Tensor.\")\n",
    "\n",
    "def create_valid_mask(pad_info, size=512):\n",
    "    \"\"\"Create binary mask (1 = valid/original area, 0 = padding).\"\"\"\n",
    "    h, w = pad_info[\"orig_size\"]\n",
    "    top, bottom, left, right = pad_info[\"pad\"]\n",
    "\n",
    "    mask = np.zeros((h + top + bottom, w + left + right), dtype=np.uint8)\n",
    "    mask[top:top+h, left:left+w] = 1\n",
    "\n",
    "    # Resize mask to match resized image size\n",
    "    mask_resized = cv2.resize(mask, (size, size), interpolation=cv2.INTER_NEAREST)\n",
    "    return torch.tensor(mask_resized, dtype=torch.float32)\n",
    "\n",
    "def undo_pad_and_resize(output, pad_info, mask=None):\n",
    "    \"\"\"\n",
    "    Reverts network output (sizeÃ—size) back to original image size\n",
    "    using the valid mask from pad_and_resize.\n",
    "    \"\"\"\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.detach().cpu().numpy()\n",
    "        if output.ndim == 3 and output.shape[0] in [1, 3]:\n",
    "            output = np.transpose(output, (1, 2, 0))\n",
    "\n",
    "    h, w = pad_info[\"orig_size\"]\n",
    "    top, bottom, left, right = pad_info[\"pad\"]\n",
    "    padded_h, padded_w = h + top + bottom, w + left + right\n",
    "\n",
    "    # Resize network output back to padded size\n",
    "    output_resized = cv2.resize(output, (padded_w, padded_h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    if mask is None:\n",
    "        mask = np.zeros((padded_h, padded_w), dtype=np.uint8)\n",
    "        mask[top:top+h, left:left+w] = 1\n",
    "    elif isinstance(mask, torch.Tensor):\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "    # Crop using mask bounding box\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    y1, y2 = ys.min(), ys.max() + 1\n",
    "    x1, x2 = xs.min(), xs.max() + 1\n",
    "    output_cropped = output_resized[y1:y2, x1:x2]\n",
    "\n",
    "    # Resize to original image size\n",
    "    output_final = cv2.resize(output_cropped, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    return output_final\n",
    "\n",
    "def undo_pad_and_resize_torch(tensor, pad_info):\n",
    "    \"\"\"\n",
    "    Undo padding and resize the tensor (B, C, H, W) back to original image size.\n",
    "    Fully differentiable version.\n",
    "    pad_info = {'orig_size': (H_orig, W_orig), 'pad': (left, top, right, bottom)}\n",
    "    \"\"\"\n",
    "    h_orig, w_orig = pad_info['orig_size']\n",
    "    top, bottom, left, right = pad_info[\"pad\"]\n",
    "    padded_h, padded_w = h_orig + top + bottom, w_orig + left + right\n",
    "\n",
    "    tensor_resized = F.interpolate(tensor, size=(padded_h, padded_w), mode='bicubic', align_corners=False)\n",
    "\n",
    "    # Remove padding explicitly using H and W axes\n",
    "    _, _, H, W = tensor_resized.shape\n",
    "    h_start = top\n",
    "    h_end = H - bottom if bottom > 0 else H\n",
    "    w_start = left\n",
    "    w_end = W - right if right > 0 else W\n",
    "\n",
    "    tensor_resized = tensor_resized[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "    # Resize back to original\n",
    "    tensor = F.interpolate(tensor_resized, size=(h_orig, w_orig), mode='bicubic', align_corners=False)\n",
    "    return tensor\n",
    "\n",
    "class PadResizeToTensor:\n",
    "    def __init__(self, size=512):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Pad & resize (handles PIL or torch tensor)\n",
    "        img_resized, pad_info = pad_and_resize(img, self.size)\n",
    "\n",
    "        # Convert mask from pad_info (single item, not batch)\n",
    "        mask = create_valid_mask(pad_info, self.size)\n",
    "        mask = mask.unsqueeze(0)  # shape (1,H,W)\n",
    "\n",
    "        # Convert tensor values to plain ints (for safe serialization)\n",
    "        pad_info = {\n",
    "            \"orig_size\": tuple(int(x) for x in pad_info[\"orig_size\"]),\n",
    "            \"pad\": tuple(int(x) for x in pad_info[\"pad\"]),\n",
    "            \"resized_size\": tuple(int(x) for x in pad_info[\"resized_size\"]),\n",
    "        }\n",
    "\n",
    "        return TF.to_tensor(img_resized), mask, pad_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a12a57",
   "metadata": {},
   "source": [
    "# 1. DualEncoderFusionNet Model (6 -> 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Conv Block\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_ch=3):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, 32)\n",
    "        self.enc2 = ConvBlock(32, 64, stride=2)\n",
    "        self.enc3 = ConvBlock(64, 128, stride=2)\n",
    "        self.enc4 = ConvBlock(128, 256, stride=2)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        return [e1, e2, e3, e4]\n",
    "\n",
    "# Attention Fusion\n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Conv2d(ch * 2, ch, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch, 2, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, f1, f2):\n",
    "        w = self.attn(torch.cat([f1, f2], dim=1))\n",
    "        return f1 * w[:, 0:1] + f2 * w[:, 1:2]\n",
    "\n",
    "# Frequency + Edge-Aware Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_ch=256, out_ch=3):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(in_ch, 128, 4, 2, 1)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.up3 = nn.ConvTranspose2d(64, 32, 4, 2, 1)\n",
    "        self.out = nn.Conv2d(32, out_ch, 3, 1, 1)\n",
    "        self.edge_enhance = nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False)\n",
    "        self.edge_enhance.weight.data.fill_(0)\n",
    "        self.edge_enhance.weight.data[:, :, 1, 1] = 1 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.up1(x))\n",
    "        x = F.relu(self.up2(x))\n",
    "        x = F.relu(self.up3(x))\n",
    "        x = torch.sigmoid(self.out(x))\n",
    "\n",
    "        # Frequency-aware enhancement: emphasize high-frequency residual\n",
    "        x_fft = fft.fftshift(fft.fft2(x, norm='ortho'))\n",
    "        mag = torch.log(1 + torch.abs(x_fft))\n",
    "        mag = mag / (torch.max(mag) + 1e-6)\n",
    "        hf_weight = 1 + 0.2 * mag\n",
    "        x = x * hf_weight\n",
    "\n",
    "        # Edge-aware enhancement\n",
    "        edge = self.edge_enhance(x)\n",
    "        return x + 0.1 * edge\n",
    "\n",
    "# Full Model\n",
    "class DualEncoderFusionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flash_enc = Encoder(3)\n",
    "        self.nonflash_enc = Encoder(3)\n",
    "        self.fusions = nn.ModuleList([\n",
    "            AttentionFusion(32),\n",
    "            AttentionFusion(64),\n",
    "            AttentionFusion(128),\n",
    "            AttentionFusion(256)\n",
    "        ])\n",
    "        self.decoder = Decoder(256, 3)\n",
    "\n",
    "    def forward(self, flash, nonflash, mask):\n",
    "        f1 = self.flash_enc(flash)\n",
    "        f2 = self.nonflash_enc(nonflash)\n",
    "        fused = [self.fusions[i](f1[i], f2[i]) for i in range(4)]\n",
    "        out = self.decoder(fused[-1])\n",
    "\n",
    "        mask_up = F.interpolate(mask, size=out.shape[2:], mode='nearest')\n",
    "        return out * mask_up\n",
    "\n",
    "fusion_model = DualEncoderFusionNet().to(device)\n",
    "fusion_model.load_state_dict(torch.load('<your_path>', map_location=device))\n",
    "fusion_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb9e57",
   "metadata": {},
   "source": [
    "# 2. UNet RGB Optimization (3 -> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.down1 = DoubleConv(3, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = DoubleConv(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.middle = DoubleConv(64, 128)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv2 = DoubleConv(128, 64)\n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.conv1 = DoubleConv(64, 32)\n",
    "\n",
    "        self.final = nn.Conv2d(32, 1, 1) # Output grayscale\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.down1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "        c2 = self.down2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "\n",
    "        m = self.middle(p2)\n",
    "\n",
    "        u2 = self.up2(m)\n",
    "        c2_cat = torch.cat([u2, c2], dim=1)\n",
    "        c2_out = self.conv2(c2_cat)\n",
    "        u1 = self.up1(c2_out)\n",
    "        c1_cat = torch.cat([u1, c1], dim=1)\n",
    "        c1_out = self.conv1(c1_cat)\n",
    "\n",
    "        return self.act(self.final(c1_out))\n",
    "    \n",
    "unet_model = UNetSmall().to(device)\n",
    "unet_model.load_state_dict(torch.load(\"<your_path>\", map_location=device))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2391d",
   "metadata": {},
   "source": [
    "# 3. TripletDeepHard (1 -> 128 dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddeb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, pretrained=False):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.backbone = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4,\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        e_a = self.forward_once(anchor)\n",
    "        e_p = self.forward_once(positive)\n",
    "        e_n = self.forward_once(negative)\n",
    "        return e_a, e_p, e_n\n",
    "    \n",
    "embedding_model = TripletNet(embedding_dim=256, pretrained=True)\n",
    "embedding_model = embedding_model.to(device)\n",
    "embedding_model.load_state_dict(torch.load(\"<your_path>\", map_location=device))\n",
    "embedding_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c22cd",
   "metadata": {},
   "source": [
    "# 4. E2E Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2EPipeline(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for contactless fingerprint embedding.\n",
    "    Accepts batched tensors (flash, nonflash, mask, pad_info) from MultiSessionFingerprintDataset.\n",
    "    Returns L2-normalized embeddings (B,128).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fusion_model, unet_model, embedding_model, device=None):\n",
    "        super().__init__()\n",
    "        self.fusion = fusion_model\n",
    "        self.unet = unet_model\n",
    "        self.embed = embedding_model\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # set models to eval by default\n",
    "        self.fusion.to(self.device).eval()\n",
    "        self.unet.to(self.device).eval()\n",
    "        self.embed.to(self.device).eval()\n",
    "\n",
    "    def forward(self, flash_t, nf_t, mask, pad_info, return_intermediate=False, with_grad=False):\n",
    "        flash_t = flash_t.to(self.device)\n",
    "        nf_t = nf_t.to(self.device)\n",
    "        mask = mask.to(self.device)\n",
    "\n",
    "        ctx = torch.enable_grad() if with_grad else torch.no_grad()\n",
    "        with ctx:\n",
    "            fused = self.fusion(flash_t, nf_t, mask)\n",
    "\n",
    "            unet_out = self.unet(fused)\n",
    "\n",
    "            unpadded_np_list = []\n",
    "            if isinstance(pad_info, dict):\n",
    "                pad_info = [pad_info] * unet_out.shape[0]\n",
    "\n",
    "            for i, info in enumerate(pad_info):\n",
    "                unpadded_t = undo_pad_and_resize_torch(unet_out[i:i+1], info)\n",
    "                unpadded_np_list.append(unpadded_t)\n",
    "\n",
    "            # Convert to embeddings\n",
    "            emb_list = []\n",
    "            for up_t in unpadded_np_list:\n",
    "                up_t_resized = F.interpolate(up_t, size=(512, 512), mode='bicubic', align_corners=False)\n",
    "                up_t_resized = up_t_resized.clamp(0.0, 1.0)\n",
    "                up_t_resized = (up_t_resized - 0.5) / 0.5\n",
    "\n",
    "                if hasattr(self.embed, \"forward_once\"):\n",
    "                    emb = self.embed.forward_once(up_t_resized)\n",
    "                else:\n",
    "                    emb = self.embed(up_t_resized)\n",
    "\n",
    "                emb = F.normalize(emb, p=2, dim=1)\n",
    "                emb_list.append(emb)\n",
    "\n",
    "            emb = torch.cat(emb_list, dim=0) \n",
    "\n",
    "        if not with_grad:  # detach only during eval/inference\n",
    "            emb = emb.detach().cpu()\n",
    "\n",
    "        if return_intermediate:\n",
    "            return {\n",
    "                \"embedding\": emb,\n",
    "                \"fused\": fused.detach().cpu() if not with_grad else fused,\n",
    "                \"unet\": unet_out.detach().cpu() if not with_grad else unet_out,\n",
    "            }\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def to_device(self, device):\n",
    "        self.device = device\n",
    "        self.fusion.to(device)\n",
    "        self.unet.to(device)\n",
    "        self.embed.to(device)\n",
    "        return self\n",
    "\n",
    "    def set_trainable(self, module_names_or_objs, trainable=True):\n",
    "        for m in module_names_or_objs:\n",
    "            if isinstance(m, str):\n",
    "                mod = getattr(self, m)\n",
    "            else:\n",
    "                mod = m\n",
    "            for p in mod.parameters():\n",
    "                p.requires_grad = trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207d040",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSessionFingerprintDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads paired flash/nonflash fingerprint images for two sessions (S1, S2).\n",
    "    Each image is processed into:\n",
    "        - tensor: (3,H,W)\n",
    "        - mask: (1,H,W)\n",
    "        - pad_info: dict\n",
    "    Returns a dict:\n",
    "        {\n",
    "            \"s1\": {\"flash\": t, \"nonflash\": t, \"mask\": t, \"pad_info\": dict},\n",
    "            \"s2\": {\"flash\": t, \"nonflash\": t, \"mask\": t, \"pad_info\": dict},\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.s1_flash_dir = os.path.join(root_dir, \"<path>\")\n",
    "        self.s1_nonflash_dir = os.path.join(root_dir, \"<path>\")\n",
    "        self.s2_flash_dir = os.path.join(root_dir, \"<path>\")\n",
    "        self.s2_nonflash_dir = os.path.join(root_dir, \"<path>\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        ids = [\n",
    "            f for f in os.listdir(self.s1_flash_dir)\n",
    "            if os.path.exists(os.path.join(self.s2_flash_dir, f))\n",
    "        ]\n",
    "        self.image_ids = sorted(ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_ids[idx]\n",
    "\n",
    "        # Load PIL images\n",
    "        s1_f = Image.open(os.path.join(self.s1_flash_dir, fname)).convert(\"RGB\")\n",
    "        s1_nf = Image.open(os.path.join(self.s1_nonflash_dir, fname)).convert(\"RGB\")\n",
    "        s2_f = Image.open(os.path.join(self.s2_flash_dir, fname)).convert(\"RGB\")\n",
    "        s2_nf = Image.open(os.path.join(self.s2_nonflash_dir, fname)).convert(\"RGB\")\n",
    "\n",
    "        # Apply transform (PadResizeToTensor returns (tensor, mask, pad_info))\n",
    "        s1_f_t, s1_f_mask, s1_f_info = self.transform(s1_f)\n",
    "        s1_nf_t, s1_nf_mask, s1_nf_info = self.transform(s1_nf)\n",
    "        s2_f_t, s2_f_mask, s2_f_info = self.transform(s2_f)\n",
    "        s2_nf_t, s2_nf_mask, s2_nf_info = self.transform(s2_nf)\n",
    "\n",
    "        return {\n",
    "            \"s1\": {\n",
    "                \"flash\": s1_f_t,\n",
    "                \"nonflash\": s1_nf_t,\n",
    "                \"mask\": s1_f_mask * s1_nf_mask,\n",
    "                \"pad_info\": s1_f_info\n",
    "            },\n",
    "            \"s2\": {\n",
    "                \"flash\": s2_f_t,\n",
    "                \"nonflash\": s2_nf_t,\n",
    "                \"mask\": s2_f_mask * s2_nf_mask,\n",
    "                \"pad_info\": s2_f_info\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898467d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    def stack_session(session_list):\n",
    "        return {\n",
    "            \"flash\": torch.stack([s[\"flash\"] for s in session_list]),\n",
    "            \"nonflash\": torch.stack([s[\"nonflash\"] for s in session_list]),\n",
    "            \"mask\": torch.stack([s[\"mask\"] for s in session_list]),\n",
    "            \"pad_info\": [s[\"pad_info\"] for s in session_list],\n",
    "        }\n",
    "\n",
    "    s1_list = [b[\"s1\"] for b in batch]\n",
    "    s2_list = [b[\"s2\"] for b in batch]\n",
    "\n",
    "    return {\"s1\": stack_session(s1_list), \"s2\": stack_session(s2_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PadResizeToTensor(512)\n",
    "root_dir = \"<your_path>\"\n",
    "\n",
    "dataset = MultiSessionFingerprintDataset(root_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "device = \"mps\"\n",
    "lr = 1e-6\n",
    "epochs = 10\n",
    "\n",
    "# Create E2E pipeline\n",
    "pipeline = E2EPipeline(\n",
    "    fusion_model,\n",
    "    unet_model,\n",
    "    embedding_model,\n",
    "    device=device\n",
    ").to_device(device)\n",
    "\n",
    "pipeline.train()\n",
    "# Unfreeze all\n",
    "pipeline.set_trainable([pipeline.fusion, pipeline.unet, pipeline.embed], True)\n",
    "\n",
    "params = list(pipeline.fusion.parameters()) + \\\n",
    "         list(pipeline.unet.parameters()) + \\\n",
    "         list(pipeline.embed.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=1e-7)\n",
    "\n",
    "\n",
    "def fine_tune_loss(emb1, emb2, margin=0.1, w_triplet=0.3, w_intra=0.3):\n",
    "    \"\"\"\n",
    "    emb1, emb2: tensors of shape [B, D], L2-normalized\n",
    "    emb1[i] and emb2[i] correspond to the same ID\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Triplet (soft margin) loss\n",
    "    sim_matrix = emb1 @ emb2.T  # cosine similarity [B, B]\n",
    "    pos_sim = sim_matrix.diagonal().unsqueeze(1)  # [B, 1]\n",
    "\n",
    "    # soft-margin version of triplet loss\n",
    "    triplet_loss = torch.log1p(torch.exp(sim_matrix - pos_sim + margin))\n",
    "    mask = ~torch.eye(sim_matrix.size(0), dtype=torch.bool, device=sim_matrix.device)\n",
    "    triplet_loss = triplet_loss[mask].mean()\n",
    "\n",
    "    # 2. Intra-cluster compactness loss\n",
    "    cos_pos = F.cosine_similarity(emb1, emb2, dim=1)\n",
    "    intra_dists = (1.0 - cos_pos).mean()   # average distance between positives\n",
    "\n",
    "    \n",
    "    # 3. Combined loss\n",
    "    total_loss = w_triplet * triplet_loss + w_intra * intra_dists\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954442be",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "save_path = \"save_path.pth\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # TRAINING\n",
    "    pipeline.train()\n",
    "    pipeline.embed.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "        s1, s2 = batch[\"s1\"], batch[\"s2\"]\n",
    "\n",
    "        emb1 = pipeline(s1[\"flash\"], s1[\"nonflash\"], s1[\"mask\"], s1[\"pad_info\"], with_grad=True)\n",
    "        emb2 = pipeline(s2[\"flash\"], s2[\"nonflash\"], s2[\"mask\"], s2[\"pad_info\"], with_grad=True)\n",
    "\n",
    "        loss =  fine_tune_loss(emb1, emb2, margin=0.08, w_triplet=0.75, w_intra=0.25)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # VALIDATION\n",
    "    pipeline.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "            s1, s2 = batch[\"s1\"], batch[\"s2\"]\n",
    "            emb1 = pipeline(s1[\"flash\"], s1[\"nonflash\"], s1[\"mask\"], s1[\"pad_info\"], with_grad=False)\n",
    "            emb2 = pipeline(s2[\"flash\"], s2[\"nonflash\"], s2[\"mask\"], s2[\"pad_info\"], with_grad=False)\n",
    "            val_loss += fine_tune_loss(emb1, emb2, margin=0.08, w_triplet=0.75, w_intra=0.25).item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_train_loss:.6f} Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # SAVE BEST MODEL\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"pipeline_state_dict\": pipeline.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "        }, save_path)\n",
    "        print(f\"âœ… Saved new best model with Val Loss: {best_val_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nðŸ Training complete. Best Val Loss: {best_val_loss:.6f}\")\n",
    "print(f\"Best fine-tuned pipeline saved to: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c82d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Val Loss')\n",
    "plt.title(\"Fine-tuning Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0956f19",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PadResizeToTensor(512)\n",
    "root_dir = \"<your_path>\"\n",
    "\n",
    "dataset = MultiSessionFingerprintDataset(root_dir, transform=transform)\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=4, shuffle=False, drop_last=False, collate_fn=custom_collate)\n",
    "pipeline = E2EPipeline(\n",
    "    fusion_model,\n",
    "    unet_model,\n",
    "    embedding_model,\n",
    "    device=device\n",
    ").to_device(device)\n",
    "\n",
    "checkpoint = torch.load(\"<your_path>\", map_location=device)\n",
    "pipeline.load_state_dict(checkpoint[\"pipeline_state_dict\"])\n",
    "pipeline.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651914d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_image_embeddings = []\n",
    "s2_image_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        s1, s2 = batch[\"s1\"], batch[\"s2\"]\n",
    "        emb1 = pipeline(s1[\"flash\"], s1[\"nonflash\"], s1[\"mask\"], s1[\"pad_info\"], with_grad=False)\n",
    "        emb2 = pipeline(s2[\"flash\"], s2[\"nonflash\"], s2[\"mask\"], s2[\"pad_info\"], with_grad=False)\n",
    "\n",
    "        s1_image_embeddings.append(emb1.cpu().numpy())\n",
    "        s2_image_embeddings.append(emb2.cpu().numpy())\n",
    "\n",
    "image_embeddings_s1 = np.vstack(s1_image_embeddings)\n",
    "image_embeddings_s2 =np.vstack(s2_image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71306053",
   "metadata": {},
   "source": [
    "### Note: Can perform required tests or plots with embeddings arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.array(image_embeddings_s1)\n",
    "s2 = np.array(image_embeddings_s2)\n",
    "N = s1.shape[0]\n",
    "\n",
    "# Genuine scores (diagonal)\n",
    "genuine_scores = np.array([np.dot(s1[i], s2[i]) / \n",
    "                           (np.linalg.norm(s1[i]) * np.linalg.norm(s2[i])) for i in range(N)])\n",
    "\n",
    "# Imposter scores (random mismatched pairs)\n",
    "num_imposters = min(1000, N*(N-1))  # sample 1000 imposter pairs\n",
    "imposter_indices1 = np.random.choice(N, num_imposters)\n",
    "imposter_indices2 = np.random.choice(N, num_imposters)\n",
    "# ensure different pairs\n",
    "mask = imposter_indices1 != imposter_indices2\n",
    "imposter_indices1 = imposter_indices1[mask]\n",
    "imposter_indices2 = imposter_indices2[mask]\n",
    "\n",
    "imposter_scores = np.array([np.dot(s1[i1], s2[i2]) /\n",
    "                            (np.linalg.norm(s1[i1]) * np.linalg.norm(s2[i2])) \n",
    "                            for i1, i2 in zip(imposter_indices1, imposter_indices2)])\n",
    "\n",
    "y_scores = np.concatenate([genuine_scores, imposter_scores])\n",
    "y_true = np.concatenate([np.ones(len(genuine_scores)), np.zeros(len(imposter_scores))])\n",
    "\n",
    "# Compute metrics\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "frr = 1 - tpr\n",
    "far = fpr\n",
    "\n",
    "# Compute EER\n",
    "eer_idx = np.argmin(np.abs(frr - far))\n",
    "eer = (frr[eer_idx] + far[eer_idx]) / 2\n",
    "eer_threshold = thresholds[eer_idx]\n",
    "print(f\"Equal Error Rate (EER): {eer:.4f} at threshold {eer_threshold:.4f}\")\n",
    "\n",
    "results = [{\n",
    "    \"fpr\": fpr,\n",
    "    \"tpr\": tpr,\n",
    "    \"fnr\": frr,\n",
    "    \"auc\": roc_auc,\n",
    "    \"eer\": eer,\n",
    "    \"eer_idx\": eer_idx,\n",
    "    \"idx\": 0\n",
    "}]\n",
    "labels_name = {0: \"Model\"}\n",
    "colors = {0: \"tab:blue\"}\n",
    "eps = 1e-6  # small offset to avoid log(0)\n",
    "\n",
    "# Plot ROC and FRR vs FAR side-by-side\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC (log scale x-axis)\n",
    "plt.subplot(1, 2, 1)\n",
    "for r in results:\n",
    "    plt.plot(r[\"fpr\"] + eps, r[\"tpr\"], color=colors[r[\"idx\"]],\n",
    "             label=f\"{labels_name[r['idx']]} (AUC={r['auc']:.3f})\")\n",
    "    plt.scatter(r[\"fpr\"][r[\"eer_idx\"]] + eps, r[\"tpr\"][r[\"eer_idx\"]],\n",
    "                color=colors[r[\"idx\"]], s=30)\n",
    "plt.xscale('log')\n",
    "plt.xlim(1e-3, 1.0)\n",
    "plt.xlabel(\"False Acceptance Rate (FAR, log scale)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curve (log scale)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.3)\n",
    "\n",
    "# FRR vs FAR (log-log scale)\n",
    "plt.subplot(1, 2, 2)\n",
    "for r in results:\n",
    "    plt.plot(r[\"fpr\"] + eps, r[\"fnr\"] + eps, color=colors[r[\"idx\"]],\n",
    "             label=f\"{labels_name[r['idx']]} (EER={r['eer']*100:.2f}%)\")\n",
    "    plt.scatter(r[\"fpr\"][r[\"eer_idx\"]] + eps, r[\"fnr\"][r[\"eer_idx\"]] + eps,\n",
    "                color=colors[r[\"idx\"]], s=30)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(1e-3, 1.0)\n",
    "plt.ylim(1e-3, 1.0)\n",
    "plt.xlabel(\"False Acceptance Rate (FAR, log scale)\")\n",
    "plt.ylabel(\"False Rejection Rate (FRR, log scale)\")\n",
    "plt.title(\"FRR vs FAR (log-log scale)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
